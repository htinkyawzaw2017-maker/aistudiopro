import warnings
warnings.filterwarnings("ignore")
import os
os.environ["GRPC_VERBOSITY"] = "ERROR"
os.environ["GLOG_minloglevel"] = "2"

import streamlit as st
import google.generativeai as genai
import edge_tts
import subprocess
import shutil
import whisper
import time
import json
import re
import requests
import textwrap
import math
import uuid
import streamlit.components.v1 as components
from google.cloud import texttospeech
from google.oauth2 import service_account

# ---------------------------------------------------------
# ğŸ›¡ï¸ 1. SESSION & FOLDER ISOLATION
# ---------------------------------------------------------
if 'session_id' not in st.session_state:
Â  Â  st.session_state.session_id = uuid.uuid4().hex

SID = st.session_state.session_id
BASE_WORK_DIR = os.path.abspath("user_sessions")
USER_SESSION_DIR = os.path.join(BASE_WORK_DIR, SID)
os.makedirs(USER_SESSION_DIR, exist_ok=True)

# File Paths
FILE_INPUT = os.path.join(USER_SESSION_DIR, "input_video.mp4")
FILE_AUDIO_RAW = os.path.join(USER_SESSION_DIR, "extracted_audio.wav")
FILE_VOICE = os.path.join(USER_SESSION_DIR, "ai_voice.mp3")
FILE_VIDEO_FREEZE = os.path.join(USER_SESSION_DIR, "video_frozen.mp4") # For Freeze Effect
FILE_FINAL = os.path.join(USER_SESSION_DIR, "final_dubbed_video.mp4")

FILE_CAP_INPUT = os.path.join(USER_SESSION_DIR, "caption_input_video.mp4")
FILE_CAP_WAV = os.path.join(USER_SESSION_DIR, "caption_audio.wav")
FILE_CAP_FINAL = os.path.join(USER_SESSION_DIR, "captioned_output.mp4")
FILE_ASS = os.path.join(USER_SESSION_DIR, "subtitles.ass")

# ---------------------------------------------------------
# ğŸ¨ UI SETUP (White Theme, Red Buttons, Mobile Fix)
# ---------------------------------------------------------
st.set_page_config(page_title="Myanmar AI Studio Pro", page_icon="ğŸ¬", layout="wide", initial_sidebar_state="expanded")

# ğŸ”¥ 1. KEEP SCREEN AWAKE (JavaScript)
keep_awake_js = """
<script>
async function requestWakeLock() {
Â  try {
Â  Â  const wakeLock = await navigator.wakeLock.request('screen');
Â  Â  console.log('Wake Lock is active! Screen will not sleep.');
Â  } catch (err) {
Â  Â  console.log(`Wake Lock Error: ${err.name}, ${err.message}`);
Â  }
}
requestWakeLock();
// Re-request wake lock if visibility changes (e.g. switching tabs)
document.addEventListener('visibilitychange', async () => {
Â  if (document.visibilityState === 'visible') {
Â  Â  requestWakeLock();
Â  }
});
</script>
"""
components.html(keep_awake_js, height=0, width=0)

# ğŸ”¥ 2. CUSTOM CSS (White Background, Red Buttons, Responsive Header)
st.markdown("""
Â  Â  <style>
Â  Â  /* Force White Background & Black Text */
Â  Â  .stApp {
Â  Â  Â  Â  background-color: #FFFFFF !important;
Â  Â  Â  Â  color: #000000 !important;
Â  Â  }
Â  Â Â 
Â  Â  /* Hide Default Header */
Â  Â  header[data-testid="stHeader"] {
Â  Â  Â  Â  visibility: hidden;
Â  Â  }
Â  Â Â 
Â  Â  /* Sidebar Styling */
Â  Â  [data-testid="stSidebar"] {
Â  Â  Â  Â  background-color: #F8F9FA;
Â  Â  Â  Â  border-right: 2px solid #FF0000;
Â  Â  }
Â  Â  [data-testid="stSidebar"] * {
Â  Â  Â  Â  color: #000000 !important;
Â  Â  }
Â  Â Â 
Â  Â  /* ğŸ”´ RED BUTTONS STYLE */
Â  Â  .stButton > button {
Â  Â  Â  Â  background: linear-gradient(45deg, #FF0000, #D90000) !important;
Â  Â  Â  Â  color: white !important;
Â  Â  Â  Â  border: none !important;
Â  Â  Â  Â  border-radius: 8px;
Â  Â  Â  Â  font-weight: bold;
Â  Â  Â  Â  box-shadow: 0px 4px 10px rgba(255, 0, 0, 0.3);
Â  Â  Â  Â  transition: all 0.3s ease;
Â  Â  }
Â  Â  .stButton > button:hover {
Â  Â  Â  Â  transform: scale(1.02);
Â  Â  Â  Â  box-shadow: 0px 6px 15px rgba(255, 0, 0, 0.5);
Â  Â  }

Â  Â  /* Sliders Red */
Â  Â  div[data-baseweb="slider"] div {
Â  Â  Â  Â  background-color: #FF0000 !important;
Â  Â  }

Â  Â  /* ğŸ“± RESPONSIVE HEADER FOR MOBILE */
Â  Â  .header-container {
Â  Â  Â  Â  display: flex;
Â  Â  Â  Â  flex-direction: row;
Â  Â  Â  Â  align-items: center;
Â  Â  Â  Â  justify-content: center;
Â  Â  Â  Â  gap: 15px;
Â  Â  Â  Â  padding: 20px;
Â  Â  Â  Â  border-bottom: 2px solid #FF0000;
Â  Â  Â  Â  margin-bottom: 20px;
Â  Â  }
Â  Â Â 
Â  Â  .header-icon {
Â  Â  Â  Â  width: 80px;
Â  Â  Â  Â  height: 80px;
Â  Â  }
Â  Â Â 
Â  Â  .header-text {
Â  Â  Â  Â  font-family: 'Orbitron', sans-serif;
Â  Â  Â  Â  color: #FF0000; /* Red Text */
Â  Â  Â  Â  font-size: 2.5rem;
Â  Â  Â  Â  font-weight: 800;
Â  Â  Â  Â  margin: 0;
Â  Â  Â  Â  line-height: 1.2;
Â  Â  }

Â  Â  /* Mobile Media Query */
Â  Â  @media only screen and (max-width: 600px) {
Â  Â  Â  Â  .header-container {
Â  Â  Â  Â  Â  Â  flex-direction: column; /* Stack vertically on phone */
Â  Â  Â  Â  Â  Â  gap: 10px;
Â  Â  Â  Â  Â  Â  padding: 10px;
Â  Â  Â  Â  }
Â  Â  Â  Â  .header-icon {
Â  Â  Â  Â  Â  Â  width: 50px;
Â  Â  Â  Â  Â  Â  height: 50px;
Â  Â  Â  Â  }
Â  Â  Â  Â  .header-text {
Â  Â  Â  Â  Â  Â  font-size: 1.8rem; /* Smaller text for mobile */
Â  Â  Â  Â  Â  Â  text-align: center;
Â  Â  Â  Â  }
Â  Â  }
Â  Â  </style>
Â  Â  <link href="https://fonts.googleapis.com/css2?family=Orbitron:wght@400;700&display=swap" rel="stylesheet">
""", unsafe_allow_html=True)

# Responsive Header HTML
st.markdown("""
<div class="header-container">
Â  Â  <img src="https://img.icons8.com/color/96/movie-projector.png" class="header-icon"/>
Â  Â  <h1 class="header-text">MYANMAR AI STUDIO</h1>
</div>
""", unsafe_allow_html=True)

# ---------------------------------------------------------
# ğŸ’¾ STATE MANAGEMENT
# ---------------------------------------------------------
if 'raw_transcript' not in st.session_state: st.session_state.raw_transcript = ""
if 'final_script' not in st.session_state: st.session_state.final_script = ""
if 'processed_video_path' not in st.session_state: st.session_state.processed_video_path = None
if 'processed_audio_path' not in st.session_state: st.session_state.processed_audio_path = None
if 'caption_video_path' not in st.session_state: st.session_state.caption_video_path = None
if 'google_creds' not in st.session_state: st.session_state.google_creds = None
if 'user_api_key' not in st.session_state: st.session_state.user_api_key = ""

# ---------------------------------------------------------
# ğŸ› ï¸ HELPER FUNCTIONS
# ---------------------------------------------------------
def load_custom_dictionary():
Â  Â  dict_file = "dictionary.txt"
Â  Â  if os.path.exists(dict_file):
Â  Â  Â  Â  with open(dict_file, "r", encoding="utf-8") as f: return f.read()
Â  Â  return ""

def load_pronunciation_dict():
Â  Â  pron_file = "pronunciation.txt"
Â  Â  replacements = {}
Â  Â  if os.path.exists(pron_file):
Â  Â  Â  Â  with open(pron_file, "r", encoding="utf-8") as f:
Â  Â  Â  Â  Â  Â  for line in f:
Â  Â  Â  Â  Â  Â  Â  Â  if "=" in line and not line.startswith("#"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  parts = line.split("=")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if len(parts) == 2:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  replacements[parts[0].strip()] = parts[1].strip()
Â  Â  return replacements

def check_requirements():
Â  Â  if shutil.which("ffmpeg") is None:
Â  Â  Â  Â  st.error("âŒ FFmpeg is missing. Please add 'ffmpeg' to packages.txt")
Â  Â  Â  Â  st.stop()

def get_duration(path):
Â  Â  try:
Â  Â  Â  Â  cmd = ['ffprobe', '-v', 'error', '-show_entries', 'format=duration', '-of', 'json', path]
Â  Â  Â  Â  r = subprocess.run(cmd, capture_output=True, text=True)
Â  Â  Â  Â  return float(json.loads(r.stdout)['format']['duration'])
Â  Â  except: return 0.0

def download_font():
Â  Â  font_dir = os.path.abspath("fonts_cache")
Â  Â  os.makedirs(font_dir, exist_ok=True)
Â  Â  font_path = os.path.join(font_dir, "Padauk-Bold.ttf")
Â  Â  if not os.path.exists(font_path):
Â  Â  Â  Â  url = "https://github.com/googlefonts/padauk/raw/main/fonts/ttf/Padauk-Bold.ttf"
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  r = requests.get(url, timeout=10)
Â  Â  Â  Â  Â  Â  with open(font_path, 'wb') as f: f.write(r.content)
Â  Â  Â  Â  except: pass
Â  Â  return font_path

def load_whisper_safe():
Â  Â  try: return whisper.load_model("base")
Â  Â  except Exception as e: st.error(f"Whisper Error (Try reloading): {e}"); return None

# ---------------------------------------------------------
# â„ï¸ FREEZE EFFECT LOGIC (The New Feature)
# ---------------------------------------------------------
def process_video_with_freeze(input_path, output_path, interval_sec, freeze_duration=3.0):
Â  Â  """
Â  Â  Cuts video into segments of 'interval_sec'.
Â  Â  Appends a 'freeze_duration' static clip of the last frame to each segment.
Â  Â  Result: Video pauses visually, but output duration increases.Â 
Â  Â  Audio will be replaced by TTS later, so original audio sync doesn't matter here.
Â  Â  """
Â  Â  if interval_sec <= 0:
Â  Â  Â  Â  shutil.copy(input_path, output_path)
Â  Â  Â  Â  return True

Â  Â  try:
Â  Â  Â  Â  total_duration = get_duration(input_path)
Â  Â  Â  Â  current_time = 0.0
Â  Â  Â  Â  segment_idx = 0
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Temp folder for chunks
Â  Â  Â  Â  temp_dir = os.path.join(USER_SESSION_DIR, "freeze_chunks")
Â  Â  Â  Â  if os.path.exists(temp_dir): shutil.rmtree(temp_dir)
Â  Â  Â  Â  os.makedirs(temp_dir, exist_ok=True)

Â  Â  Â  Â  concat_list_path = os.path.join(temp_dir, "concat_freeze.txt")
Â  Â  Â  Â Â 
Â  Â  Â  Â  with open(concat_list_path, "w") as f:
Â  Â  Â  Â  Â  Â  while current_time < total_duration:
Â  Â  Â  Â  Â  Â  Â  Â  # 1. Calculate duration for this segment
Â  Â  Â  Â  Â  Â  Â  Â  duration = min(interval_sec, total_duration - current_time)
Â  Â  Â  Â  Â  Â  Â  Â  seg_file = os.path.join(temp_dir, f"seg_{segment_idx}.mp4")
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Extract segment
Â  Â  Â  Â  Â  Â  Â  Â  # -an: Remove audio (since we replace it anyway, and it makes concat easier)
Â  Â  Â  Â  Â  Â  Â  Â  subprocess.run([
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'ffmpeg', '-y', '-ss', str(current_time), '-t', str(duration),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  '-i', input_path, '-an', '-c:v', 'libx264', '-preset', 'ultrafast', seg_file
Â  Â  Â  Â  Â  Â  Â  Â  ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  f.write(f"file '{seg_file}'\n")
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # 2. Create Freeze Frame (if we are not at the absolute end, or user wants it)
Â  Â  Â  Â  Â  Â  Â  Â  # Logic: We freeze AFTER each interval.
Â  Â  Â  Â  Â  Â  Â  Â  if current_time + duration <= total_duration:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  freeze_file = os.path.join(temp_dir, f"freeze_{segment_idx}.mp4")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  last_frame_img = os.path.join(temp_dir, f"frame_{segment_idx}.jpg")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Extract last frame
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  subprocess.run([
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'ffmpeg', '-y', '-sseof', '-0.1', '-i', seg_file,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  '-update', '1', '-q:v', '2', last_frame_img
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Create static video from image
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  subprocess.run([
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'ffmpeg', '-y', '-loop', '1', '-i', last_frame_img,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  '-t', str(freeze_duration), '-c:v', 'libx264', '-preset', 'ultrafast',
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  '-pix_fmt', 'yuv420p', freeze_file
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f.write(f"file '{freeze_file}'\n")
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  current_time += duration
Â  Â  Â  Â  Â  Â  Â  Â  segment_idx += 1

Â  Â  Â  Â  # 3. Concatenate all segments
Â  Â  Â  Â  subprocess.run([
Â  Â  Â  Â  Â  Â  'ffmpeg', '-y', '-f', 'concat', '-safe', '0', '-i', concat_list_path,
Â  Â  Â  Â  Â  Â  '-c', 'copy', output_path
Â  Â  Â  Â  ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
Â  Â  Â  Â Â 
Â  Â  Â  Â  return True
Â  Â  except Exception as e:
Â  Â  Â  Â  print(f"Freeze Error: {e}")
Â  Â  Â  Â  return False

# ---------------------------------------------------------
# ğŸ”Š AUDIO ENGINE
# ---------------------------------------------------------
VOICE_MAP = {
Â  Â  "Burmese": {"Male": "my-MM-ThihaNeural", "Female": "my-MM-NilarNeural"},
Â  Â  "English": {"Male": "en-US-ChristopherNeural", "Female": "en-US-AriaNeural"},
}
GOOGLE_VOICE_MAP = {
Â  Â  "Burmese": {"Male": "my-MM-Standard-A", "Female": "my-MM-Standard-A"},Â 
Â  Â  "English": {"Male": "en-US-Neural2-D", "Female": "en-US-Neural2-F"},
}
VOICE_MODES = {
Â  Â  "Normal": {"rate": "+0%", "pitch": "+0Hz"},
Â  Â  "Story": {"rate": "-5%", "pitch": "-2Hz"},Â 
Â  Â  "Recap": {"rate": "+5%", "pitch": "+0Hz"},
Â  Â  "Motivation": {"rate": "+10", "pitch": "+2Hz"},
}
EMOTION_MAP = {
Â  Â  "[normal]": {"rate": "+0%", "pitch": "+0Hz"},
Â  Â  "[sad]": Â  Â {"rate": "-15%", "pitch": "-15Hz"},
Â  Â  "[angry]": Â {"rate": "+15%", "pitch": "+5Hz"},
Â  Â  "[happy]": Â {"rate": "+10%", "pitch": "+15Hz"},
Â  Â  "[action]": {"rate": "+30%", "pitch": "+0Hz"},
Â  Â  "[whisper]": {"rate": "-10%", "pitch": "-20Hz"},
}

def generate_edge_chunk(text, lang, gender, rate_str, pitch_str, output_file):
Â  Â  voice_id = VOICE_MAP.get(lang, {}).get(gender, "en-US-AriaNeural")
Â  Â  cmd = ["edge-tts", "--voice", voice_id, "--text", text, f"--rate={rate_str}", f"--pitch={pitch_str}", "--write-media", output_file]
Â  Â  for attempt in range(3):
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  subprocess.run(cmd, check=True, stdout=subprocess.DEVNULL, stderr=subprocess.PIPE)
Â  Â  Â  Â  Â  Â  if os.path.exists(output_file) and os.path.getsize(output_file) > 0: return True
Â  Â  Â  Â  except: time.sleep(1); continue
Â  Â  return False

def generate_google_chunk(text, lang, gender, rate_val, pitch_val, output_file, creds):
Â  Â  try:
Â  Â  Â  Â  client = texttospeech.TextToSpeechClient(credentials=creds)
Â  Â  Â  Â  s_input = texttospeech.SynthesisInput(text=text)
Â  Â  Â  Â  g_voice_name = GOOGLE_VOICE_MAP.get(lang, {}).get(gender, "en-US-Neural2-F")
Â  Â  Â  Â  lang_code = "my-MM" if lang == "Burmese" else "en-US"
Â  Â  Â  Â  voice = texttospeech.VoiceSelectionParams(language_code=lang_code, name=g_voice_name)
Â  Â  Â  Â  audio_config = texttospeech.AudioConfig(audio_encoding=texttospeech.AudioEncoding.MP3, speaking_rate=rate_val, pitch=pitch_val)
Â  Â  Â  Â  response = client.synthesize_speech(input=s_input, voice=voice, audio_config=audio_config)
Â  Â  Â  Â  with open(output_file, "wb") as out: out.write(response.audio_content)
Â  Â  Â  Â  return True
Â  Â  except Exception as e: print(f"Google TTS Error: {e}"); return False

def generate_audio_with_emotions(full_text, lang, gender, base_mode, output_file, engine="Edge TTS", base_speed=1.0):
Â  Â  base_settings = VOICE_MODES.get(base_mode, VOICE_MODES["Normal"])
Â  Â  base_r_int = int(base_settings['rate'].replace('%', ''))
Â  Â  base_p_int = int(base_settings['pitch'].replace('Hz', ''))
Â  Â  slider_adj = int((base_speed - 1.0) * 100)
Â  Â  current_rate = base_r_int + slider_adj
Â  Â  current_pitch = base_p_int

Â  Â  parts = re.split(r'(\[.*?\])', full_text)
Â  Â  audio_segments = []
Â  Â  chunk_idx = 0
Â  Â  output_dir = os.path.dirname(output_file)
Â  Â Â 
Â  Â  for part in parts:
Â  Â  Â  Â  part = part.strip()
Â  Â  Â  Â  if not part: continue
Â  Â  Â  Â  part_lower = part.lower()

Â  Â  Â  Â  if part_lower == "[p]":
Â  Â  Â  Â  Â  Â  chunk_filename = os.path.join(output_dir, f"chunk_{chunk_idx}_silence.mp3")
Â  Â  Â  Â  Â  Â  cmd = ['ffmpeg', '-y', '-f', 'lavfi', '-i', 'anullsrc=r=24000:cl=mono', '-t', '1', '-q:a', '9', chunk_filename]
Â  Â  Â  Â  Â  Â  subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
Â  Â  Â  Â  Â  Â  if os.path.exists(chunk_filename): audio_segments.append(chunk_filename); chunk_idx += 1
Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  if part_lower in EMOTION_MAP:
Â  Â  Â  Â  Â  Â  emo = EMOTION_MAP[part_lower]
Â  Â  Â  Â  Â  Â  base_r = int(base_settings['rate'].replace('%', '')) + slider_adj
Â  Â  Â  Â  Â  Â  base_p = int(base_settings['pitch'].replace('Hz', ''))
Â  Â  Â  Â  Â  Â  current_rate = base_r + int(emo['rate'].replace('%', ''))
Â  Â  Â  Â  Â  Â  current_pitch = base_p + int(emo['pitch'].replace('Hz', ''))
Â  Â  Â  Â  Â  Â  continue
Â  Â  Â  Â Â 
Â  Â  Â  Â  if part.startswith("[") and part.endswith("]"): continue
Â  Â  Â  Â Â 
Â  Â  Â  Â  processed_text = normalize_text_for_tts(part)
Â  Â  Â  Â  if not processed_text.strip(): continue
Â  Â  Â  Â Â 
Â  Â  Â  Â  chunk_filename = os.path.join(output_dir, f"chunk_{chunk_idx}.mp3")
Â  Â  Â  Â  success = False
Â  Â  Â  Â  if engine == "Google Cloud TTS" and st.session_state.google_creds:
Â  Â  Â  Â  Â  Â  g_rate = 1.0 + (current_rate / 100.0)
Â  Â  Â  Â  Â  Â  g_pitch = current_pitch / 10.0Â 
Â  Â  Â  Â  Â  Â  success = generate_google_chunk(processed_text, lang, gender, g_rate, g_pitch, chunk_filename, st.session_state.google_creds)
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  rate_str = f"{current_rate:+d}%"
Â  Â  Â  Â  Â  Â  pitch_str = f"{current_pitch:+d}Hz"
Â  Â  Â  Â  Â  Â  success = generate_edge_chunk(processed_text, lang, gender, rate_str, pitch_str, chunk_filename)
Â  Â  Â  Â Â 
Â  Â  Â  Â  if success:
Â  Â  Â  Â  Â  Â  audio_segments.append(chunk_filename)
Â  Â  Â  Â  Â  Â  chunk_idx += 1
Â  Â  Â  Â  Â  Â  if engine == "Edge TTS": time.sleep(0.1)

Â  Â  if not audio_segments: return False, "No audio generated"
Â  Â Â 
Â  Â  concat_list = os.path.join(output_dir, "concat_list.txt")
Â  Â  with open(concat_list, "w") as f:
Â  Â  Â  Â  for seg in audio_segments: f.write(f"file '{seg}'\n")
Â  Â  Â  Â  Â  Â Â 
Â  Â  try:
Â  Â  Â  Â  subprocess.run(['ffmpeg', '-y', '-f', 'concat', '-safe', '0', '-i', concat_list, '-c', 'copy', output_file], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
Â  Â  Â  Â  return True, "Success"
Â  Â  except Exception as e: return False, str(e)

def num_to_burmese_spoken(num_str):
Â  Â  try:
Â  Â  Â  Â  num_str = num_str.replace(",", "")
Â  Â  Â  Â  n = int(num_str)
Â  Â  Â  Â  if n == 0: return "á€á€¯á€Š"
Â  Â  Â  Â  digit_map = ["", "á€á€…á€º", "á€”á€¾á€…á€º", "á€á€¯á€¶á€¸", "á€œá€±á€¸", "á€„á€«á€¸", "á€á€¼á€±á€¬á€€á€º", "á€á€¯á€”á€…á€º", "á€›á€¾á€…á€º", "á€€á€­á€¯á€¸"]
Â  Â  Â  Â  def convert_chunk(number):
Â  Â  Â  Â  Â  Â  parts = []
Â  Â  Â  Â  Â  Â  if number >= 10000000: parts.append(convert_chunk(number // 10000000) + "á€€á€¯á€‹á€±"); number %= 10000000
Â  Â  Â  Â  Â  Â  if number >= 1000000: parts.append(digit_map[number // 1000000] + "á€á€”á€ºá€¸"); number %= 1000000
Â  Â  Â  Â  Â  Â  if number >= 100000: parts.append(digit_map[number // 100000] + "á€á€­á€”á€ºá€¸"); number %= 100000
Â  Â  Â  Â  Â  Â  if number >= 10000: parts.append(digit_map[number // 10000] + "á€á€±á€¬á€„á€ºá€¸"); number %= 10000
Â  Â  Â  Â  Â  Â  if number >= 1000: parts.append(digit_map[number // 1000] + "á€‘á€±á€¬á€„á€º"); number %= 1000
Â  Â  Â  Â  Â  Â  if number >= 100: parts.append(digit_map[number // 100] + "á€›á€¬"); number %= 100
Â  Â  Â  Â  Â  Â  if number >= 10: parts.append(digit_map[number // 10] + "á€†á€šá€º"); number %= 10
Â  Â  Â  Â  Â  Â  if number > 0: parts.append(digit_map[number])
Â  Â  Â  Â  Â  Â  return "".join(parts)
Â  Â  Â  Â  result = convert_chunk(n)
Â  Â  Â  Â  result = result.replace("á€‘á€±á€¬á€„á€º", "á€‘á€±á€¬á€„á€·á€º").replace("á€›á€¬", "á€›á€¬á€·").replace("á€†á€šá€º", "á€†á€šá€·á€º")
Â  Â  Â  Â  if result.endswith("á€‘á€±á€¬á€„á€·á€º"): result = result[:-1] + "á€„á€º"
Â  Â  Â  Â  if result.endswith("á€›á€¬á€·"): result = result[:-1]
Â  Â  Â  Â  if result.endswith("á€†á€šá€·á€º"): result = result[:-1]
Â  Â  Â  Â  return result
Â  Â  except: return num_str

def normalize_text_for_tts(text):
Â  Â  if not text: return ""
Â  Â  text = re.sub(r'(?<=\d),(?=\d)', '', text)
Â  Â  text = text.replace("*", "").replace("#", "").replace("- ", "").replace('"', "").replace("'", "")
Â  Â  pron_dict = load_pronunciation_dict()
Â  Â  sorted_keys = sorted(pron_dict.keys(), key=len, reverse=True)
Â  Â  for original in sorted_keys:
Â  Â  Â  Â  text = re.compile(re.escape(original), re.IGNORECASE).sub(pron_dict[original], text)
Â  Â  text = text.replace("áŠ", ", ").replace("á‹", ". ").replace("[p]", "... ")Â 
Â  Â  text = re.sub(r'\b\d+(?:\.\d+)?\b', lambda x: num_to_burmese_spoken(x.group()), text)
Â  Â  text = re.sub(r'\s+', ' ', text).strip()
Â  Â  return text

# ---------------------------------------------------------
# ğŸ§  AI ENGINE
# ---------------------------------------------------------
def generate_content(prompt, image_input=None):
Â  Â  api_key = st.session_state.user_api_key
Â  Â  if not api_key:
Â  Â  Â  Â  return "âŒ Please enter your Gemini API Key in the sidebar first."
Â  Â Â 
Â  Â  genai.configure(api_key=api_key)
Â  Â  # Corrected Model List: Using official names
Â  Â  model_name = st.session_state.get("selected_model", "gemini-1.5-pro")
Â  Â Â 
Â  Â  try:
Â  Â  Â  Â  model = genai.GenerativeModel(model_name)
Â  Â  Â  Â  custom_rules = load_custom_dictionary()
Â  Â  Â  Â  full_prompt = f"RULES:\n{custom_rules}\n\nTASK:\n{prompt}" if custom_rules else prompt
Â  Â  Â  Â Â 
Â  Â  Â  Â  if image_input:
Â  Â  Â  Â  Â  Â  response = model.generate_content([image_input, full_prompt])
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  response = model.generate_content(full_prompt)
Â  Â  Â  Â  return response.text
Â  Â  except Exception as e:
Â  Â  Â  Â  return f"AI Error: {str(e)}"

# ---------------------------------------------------------
# ğŸ“ .ASS SUBTITLE
# ---------------------------------------------------------
def generate_ass_file(segments, font_path, output_path):
Â  Â  def seconds_to_ass(seconds):
Â  Â  Â  Â  h = int(seconds // 3600); m = int((seconds % 3600) // 60); s = int(seconds % 60); cs = int((seconds % 1) * 100)
Â  Â  Â  Â  return f"{h}:{m:02d}:{s:02d}.{cs:02d}"
Â  Â  header = f"""[Script Info]
ScriptType: v4.00+
PlayResX: 1920
PlayResY: 1080
WrapStyle: 0
[V4+ Styles]
Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding
Style: CapCut,Padauk-Bold,24,&H0000FFFF,&H000000FF,&H00000000,&H00000000,1,0,0,0,100,100,0,0,3,0,0,2,10,10,50,1
[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
"""
Â  Â  with open(output_path, "w", encoding="utf-8") as f:
Â  Â  Â  Â  f.write(header)
Â  Â  Â  Â  for seg in segments:
Â  Â  Â  Â  Â  Â  start = seconds_to_ass(seg['start'])
Â  Â  Â  Â  Â  Â  end = seconds_to_ass(seg['end'])
Â  Â  Â  Â  Â  Â  raw_text = seg['text'].strip()
Â  Â  Â  Â  Â  Â  wrapped_lines = textwrap.wrap(raw_text, width=40)
Â  Â  Â  Â  Â  Â  final_text = "\\N".join(wrapped_lines)Â 
Â  Â  Â  Â  Â  Â  f.write(f"Dialogue: 0,{start},{end},CapCut,,0,0,0,,{final_text}\n")
Â  Â  return output_path

# ---------------------------------------------------------
# ğŸ–¥ï¸ MAIN UI & SIDEBAR
# ---------------------------------------------------------
with st.sidebar:
Â  Â  st.header("âš™ï¸ SETTINGS")
Â  Â Â 
Â  Â  # API KEY INPUT
Â  Â  st.markdown("### ğŸ”‘ API Key")
Â  Â  user_key = st.text_input("Paste Gemini API Key:", type="password", help="Get a free key from Google AI Studio.")
Â  Â Â 
Â  Â  if user_key:
Â  Â  Â  Â  st.session_state.user_api_key = user_key.strip()
Â  Â  Â  Â  st.success("âœ… Connected")
Â  Â  else:
Â  Â  Â  Â  st.error("âš ï¸ Key Required")

Â  Â  st.divider()

Â  Â  # ğŸ”¥ FREEZE SETTINGS (New Feature)
Â  Â  st.markdown("### â„ï¸ Freeze Effect")
Â  Â  freeze_option = st.selectbox(
Â  Â  Â  Â  "Interval (Video stops, Audio continues)",Â 
Â  Â  Â  Â  ["No Freeze", "Every 30 Seconds", "Every 1 Minute", "Every 2 Minutes"]
Â  Â  )
Â  Â Â 
Â  Â  # Map selection to seconds
Â  Â  freeze_interval = 0
Â  Â  if freeze_option == "Every 30 Seconds": freeze_interval = 30
Â  Â  elif freeze_option == "Every 1 Minute": freeze_interval = 60
Â  Â  elif freeze_option == "Every 2 Minutes": freeze_interval = 120

Â  Â  st.divider()

Â  Â  st.markdown("â˜ï¸ **Google Cloud TTS:**")
Â  Â  gcp_file = st.file_uploader("Upload service_account.json", type=["json"])
Â  Â  if gcp_file:
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  gcp_data = json.load(gcp_file)
Â  Â  Â  Â  Â  Â  st.session_state.google_creds = service_account.Credentials.from_service_account_info(gcp_data)
Â  Â  Â  Â  Â  Â  st.success("âœ… GCP Active")
Â  Â  Â  Â  except: st.error("âŒ Invalid JSON")

Â  Â  st.divider()
Â  Â  # Corrected Model List
Â  Â  st.session_state.selected_model = st.selectbox(
Â  Â  Â  Â  "AI Model",Â 
Â  Â  Â  Â  [ "gemini-2.5-flash", "gemini-2.0-flash"],Â 
Â  Â  Â  Â  index=0
Â  Â  )

Â  Â  with st.expander("ğŸš¨ Danger Zone", expanded=False):
Â  Â  Â  Â  if st.button("ğŸ—‘ï¸ Clear My Data"):
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  if os.path.exists(USER_SESSION_DIR):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  shutil.rmtree(USER_SESSION_DIR)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  os.makedirs(USER_SESSION_DIR, exist_ok=True)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.success("Data cleared!")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  time.sleep(1)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  Â  Â  except Exception as e: st.error(str(e))

Â  Â  if st.button("ğŸ”´ Reset System", use_container_width=True):
Â  Â  Â  Â  for key in st.session_state.keys(): del st.session_state[key]
Â  Â  Â  Â  st.rerun()

# âš ï¸ STOP IF NO KEY
if not st.session_state.user_api_key:
Â  Â  st.warning("ğŸ‘‹ Welcome! Please enter your Gemini API Key in the Sidebar to start.")
Â  Â  st.stop()

t1, t2, t3 = st.tabs(["ğŸ™ï¸ DUBBING STUDIO", "ğŸ“ AUTO CAPTION", "ğŸš€ VIRAL SEO"])

# === TAB 1: DUBBING STUDIO ===
with t1:
Â  Â  col_up, col_set = st.columns([2, 1])
Â  Â  with col_up:
Â  Â  Â  Â  uploaded = st.file_uploader("Upload Video", type=['mp4','mov'], key="dub")
Â  Â  with col_set:
Â  Â  Â  Â  task_mode = st.radio("Mode", ["ğŸ—£ï¸ Translate (Dubbing)", "ğŸ‘€ AI Narration (Silent Video)"])
Â  Â  Â  Â Â 
Â  Â  Â  Â  if task_mode == "ğŸ—£ï¸ Translate (Dubbing)":
Â  Â  Â  Â  Â  Â  in_lang = st.selectbox("Input Language", ["English", "Burmese", "Japanese", "Chinese", "Thai"])
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  vibe = st.selectbox("Narration Style", ["Vlog/Casual", "Tutorial/Explainer", "Relaxing/ASMR", "Exciting/Unboxing"])
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  out_lang = st.selectbox("Output Language", ["Burmese", "English"], index=0)
Â  Â Â 
Â  Â  if uploaded:
Â  Â  Â  Â  with open(FILE_INPUT, "wb") as f: f.write(uploaded.getbuffer())
Â  Â  Â  Â Â 
Â  Â  Â  Â  if st.button("ğŸš€ Start Magic", use_container_width=True):
Â  Â  Â  Â  Â  Â  check_requirements()
Â  Â  Â  Â  Â  Â  p_bar = st.progress(0, text="Starting...")

Â  Â  Â  Â  Â  Â  # PATH A: TRANSLATION
Â  Â  Â  Â  Â  Â  if task_mode == "ğŸ—£ï¸ Translate (Dubbing)":
Â  Â  Â  Â  Â  Â  Â  Â  p_bar.progress(20, text="ğŸ¤ Listening to Audio...")
Â  Â  Â  Â  Â  Â  Â  Â  subprocess.run(['ffmpeg', '-y', '-i', FILE_INPUT, '-vn', '-acodec', 'pcm_s16le', '-ar', '16000', '-ac', '1', FILE_AUDIO_RAW], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
Â  Â  Â  Â  Â  Â  Â  Â  model = load_whisper_safe()
Â  Â  Â  Â  Â  Â  Â  Â  if model:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  lang_map = {"Burmese": "my", "English": "en", "Japanese": "ja", "Chinese": "zh", "Thai": "th"}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  lang_code = lang_map.get(in_lang, "en")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  raw = model.transcribe(FILE_AUDIO_RAW, language=lang_code)['text']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.raw_transcript = raw
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  p_bar.progress(50, text="ğŸ§  Translating...")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  recap_style_guide = """
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ROLE: You are a famous Myanmar Movie Recap Narrator.
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  TONE: Dramatic, Flowing, Suspenseful.
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  STRICT WRITING RULES:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  1. Use dramatic vocabulary ('á€™á€»á€€á€ºá€á€«á€¸á€‘á€„á€ºá€‘á€„á€º á€á€½á€±á€·á€œá€­á€¯á€€á€ºá€›á€•á€«á€á€šá€º').
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  2. Connect sentences smoothly using Cause & Effect.
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  3. End sentences naturally with 'á€•á€«á€á€±á€¬á€·á€á€šá€º', 'á€á€²á€·á€•á€«á€á€šá€º', ''.
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  4. Do not use robotic fillers.
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  """
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if in_lang == out_lang:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  prompt = f"""{recap_style_guide}\nTASK: Rewrite into flowing Recap script.\nInput: '{raw}'"""
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  prompt = f"""{recap_style_guide}\nTASK: Translate to Burmese Recap script.\nInput: '{raw}'"""
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.final_script = generate_content(prompt)

Â  Â  Â  Â  Â  Â  # PATH B: AI NARRATION (VISION)
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  p_bar.progress(20, text="ğŸ‘€ AI is watching video...")
Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  genai.configure(api_key=st.session_state.user_api_key)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  video_file = genai.upload_file(path=FILE_INPUT)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  while video_file.state.name == "PROCESSING":
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  time.sleep(2)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  video_file = genai.get_file(video_file.name)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  p_bar.progress(50, text="âœï¸ Writing Script...")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  prompt = f"""
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ROLE: Professional Video Narrator.
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  TASK: Write a voiceover script in {out_lang}.
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  STYLE: {vibe}.
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  RULES: Describe actions naturally. Match video pacing. Use engaging language.
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  """
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.final_script = generate_content(prompt, image_input=video_file)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  genai.delete_file(video_file.name)
Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"AI Vision Error: {e}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.stop()

Â  Â  Â  Â  Â  Â  p_bar.progress(100, text="âœ… Script Ready!")
Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â Â 
Â  Â  Â  Â  txt = st.text_area("Final Script", st.session_state.final_script, height=200)

Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  st.markdown("#### âš™ï¸ Rendering Options")
Â  Â  Â  Â Â 
Â  Â  Â  Â  tts_engine = st.radio("Voice Engine", ["Edge TTS (Free)", "Google Cloud TTS (Pro)"], horizontal=True)
Â  Â  Â  Â  c_fmt, c_spd = st.columns([1, 1.2])Â 
Â  Â  Â  Â  with c_fmt: export_format = st.radio("Export Format:", ["ğŸ¬ Video (MP4)", "ğŸµ Audio Only (MP3)"], horizontal=True)
Â  Â  Â  Â  with c_spd:
Â  Â  Â  Â  Â  Â  audio_speed = st.slider("ğŸ”Š Audio Speed", 0.5, 2.0, 1.0, 0.05)
Â  Â  Â  Â  Â  Â  video_speed = st.slider("ğŸï¸ Video Speed", 0.5, 4.0, 1.0, 0.1)

Â  Â  Â  Â  c_v1, c_v2, c_v3 = st.columns(3)
Â  Â  Â  Â  with c_v1: target_lang = st.selectbox("Voice Lang", list(VOICE_MAP.keys()), index=0 if out_lang == "Burmese" else 1)
Â  Â  Â  Â  with c_v2: gender = st.selectbox("Gender", ["Male", "Female"])
Â  Â  Â  Â  with c_v3: v_mode = st.selectbox("Voice Mode", list(VOICE_MODES.keys()))
Â  Â  Â  Â Â 
Â  Â  Â  Â  zoom_val = st.slider("ğŸ” Copyright Zoom (Video Only)", 1.0, 1.2, 1.0, 0.01)

Â  Â  Â  Â  btn_label = "ğŸš€ GENERATE AUDIO" if "Audio" in export_format else "ğŸš€ RENDER FINAL VIDEO"
Â  Â  Â  Â Â 
Â  Â  Â  Â  if st.button(btn_label, use_container_width=True):
Â  Â  Â  Â  Â  Â  p_bar = st.progress(0, text="ğŸš€ Initializing...")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if not txt.strip(): st.error("âŒ Script is empty!"); st.stop()

Â  Â  Â  Â  Â  Â  p_bar.progress(30, text="ğŸ”Š Generating Speech...")
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  success, msg = generate_audio_with_emotions(txt, target_lang, gender, v_mode, FILE_VOICE, engine=tts_engine, base_speed=audio_speed)
Â  Â  Â  Â  Â  Â  Â  Â  if not success: st.error(f"âŒ Audio Failed: {msg}"); st.stop()
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.processed_audio_path = FILE_VOICE
Â  Â  Â  Â  Â  Â  except Exception as e: st.error(f"Audio Error: {e}"); st.stop()
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if "Audio" in export_format:
Â  Â  Â  Â  Â  Â  Â  Â  p_bar.progress(100, text="âœ… Done!")
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  p_bar.progress(50, text="ğŸï¸ Rendering Video (Applying Effects)...")
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # â„ï¸ APPLY FREEZE EFFECT IF SELECTED
Â  Â  Â  Â  Â  Â  Â  Â  video_source = FILE_INPUT
Â  Â  Â  Â  Â  Â  Â  Â  if freeze_interval > 0:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.toast(f"â„ï¸ Freezing video every {freeze_option}...")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  freeze_success = process_video_with_freeze(FILE_INPUT, FILE_VIDEO_FREEZE, freeze_interval, freeze_duration=3.0)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if freeze_success:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  video_source = FILE_VIDEO_FREEZE
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.warning("Freeze effect failed, using original video.")

Â  Â  Â  Â  Â  Â  Â  Â  pts_val = 1.0 / video_speed
Â  Â  Â  Â  Â  Â  Â  Â  w_s = int(1920 * zoom_val); h_s = int(1080 * zoom_val)
Â  Â  Â  Â  Â  Â  Â  Â  if w_s % 2 != 0: w_s += 1
Â  Â  Â  Â  Â  Â  Â  Â  if h_s % 2 != 0: h_s += 1
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  aud_dur = get_duration(FILE_VOICE)
Â  Â  Â  Â  Â  Â  Â  Â  vid_dur = get_duration(video_source) / video_speed
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Logic: If audio is longer, loop video or freeze last frame.
Â  Â  Â  Â  Â  Â  Â  Â  # Here we use -stream_loop -1 and -shortest to ensure video matches audio length
Â  Â  Â  Â  Â  Â  Â  Â  cmd = ['ffmpeg', '-y', '-stream_loop', '-1', '-i', video_source, '-i', FILE_VOICE,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â '-filter_complex', f"[0:v]setpts={pts_val}*PTS,scale={w_s}:{h_s},crop=1920:1080[vzoom]",Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â '-map', '[vzoom]', '-map', '1:a', '-c:v', 'libx264', '-preset', 'ultrafast', '-c:a', 'aac',Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â '-shortest', FILE_FINAL]

Â  Â  Â  Â  Â  Â  Â  Â  subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
Â  Â  Â  Â  Â  Â  Â  Â  if os.path.exists(FILE_FINAL) and os.path.getsize(FILE_FINAL) > 1000:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.processed_video_path = FILE_FINAL
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  p_bar.progress(100, text="ğŸ‰ Done!")
Â  Â  Â  Â  Â  Â  Â  Â  else: st.error("âŒ Video Generation Failed")

Â  Â  if st.session_state.processed_video_path and "Video" in export_format:
Â  Â  Â  Â  st.video(st.session_state.processed_video_path)
Â  Â  Â  Â  with open(st.session_state.processed_video_path, "rb") as f: st.download_button("ğŸ¬ Download Video", f, "dubbed.mp4", use_container_width=True)

Â  Â  if st.session_state.processed_audio_path:
Â  Â  Â  Â  st.audio(st.session_state.processed_audio_path)
Â  Â  Â  Â  with open(st.session_state.processed_audio_path, "rb") as f: st.download_button("ğŸµ Download Audio", f, "voice.mp3", use_container_width=True)

# === TAB 2: AUTO CAPTION ===
with t2:
Â  Â  st.subheader("ğŸ“ Auto Caption")
Â  Â  cap_up = st.file_uploader("Upload Video", type=['mp4','mov'], key="cap")
Â  Â  if cap_up:
Â  Â  Â  Â  with open(FILE_CAP_INPUT, "wb") as f: f.write(cap_up.getbuffer())
Â  Â  Â  Â  if st.button("Generate Captions", use_container_width=True):
Â  Â  Â  Â  Â  Â  check_requirements(); font_path = download_font()
Â  Â  Â  Â  Â  Â  p_bar = st.progress(0, text="Processing...")
Â  Â  Â  Â  Â  Â  subprocess.run(['ffmpeg', '-y', '-i', FILE_CAP_INPUT, '-vn', '-acodec', 'pcm_s16le', '-ar', '16000', '-ac', '1', FILE_CAP_WAV], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
Â  Â  Â  Â  Â  Â  model = load_whisper_safe()
Â  Â  Â  Â  Â  Â  if model:
Â  Â  Â  Â  Â  Â  Â  Â  segments = model.transcribe(FILE_CAP_WAV, task="transcribe")['segments']
Â  Â  Â  Â  Â  Â  Â  Â  trans_segments = []
Â  Â  Â  Â  Â  Â  Â  Â  for i, seg in enumerate(segments):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  p_bar.progress(int((i/len(segments))*50), text=f"ğŸ§  Translating...")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  txt = seg['text'].strip()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if txt:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  burmese = generate_content(f"Translate to Burmese. Short. Input: '{txt}'")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  trans_segments.append({'start': seg['start'], 'end': seg['end'], 'text': burmese})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  time.sleep(0.3)
Â  Â  Â  Â  Â  Â  Â  Â  p_bar.progress(90, text="âœï¸ Burning Subtitles...")
Â  Â  Â  Â  Â  Â  Â  Â  generate_ass_file(trans_segments, font_path, FILE_ASS)
Â  Â  Â  Â  Â  Â  Â  Â  font_dir = os.path.dirname(font_path)
Â  Â  Â  Â  Â  Â  Â  Â  subprocess.run(['ffmpeg', '-y', '-i', FILE_CAP_INPUT, '-vf', f"ass={FILE_ASS}:fontsdir={font_dir}", '-c:a', 'copy', '-c:v', 'libx264', '-preset', 'ultrafast', FILE_CAP_FINAL], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
Â  Â  Â  Â  Â  Â  Â  Â  if os.path.exists(FILE_CAP_FINAL):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state.caption_video_path = FILE_CAP_FINAL
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  p_bar.progress(100, text="Done!")

Â  Â  if st.session_state.caption_video_path:
Â  Â  Â  Â  st.video(st.session_state.caption_video_path)
Â  Â  Â  Â  with open(st.session_state.caption_video_path, "rb") as f: st.download_button("Download", f, "captioned.mp4", use_container_width=True)

# === TAB 3: VIRAL SEO ===
with t3:
Â  Â  st.subheader("ğŸš€ Viral Kit SEO")
Â  Â  if st.session_state.final_script:
Â  Â  Â  Â  if st.button("Generate Metadata", use_container_width=True):
Â  Â  Â  Â  Â  Â  with st.spinner("Analyzing..."):
Â  Â  Â  Â  Â  Â  Â  Â  prompt = f"""Based on: {st.session_state.final_script}\nGenerate:\n1. 5 Clickbait Titles (Burmese)\n2. 10 Hashtags\n3. Description"""
Â  Â  Â  Â  Â  Â  Â  Â  seo_result = generate_content(prompt)
Â  Â  Â  Â  Â  Â  Â  Â  st.success("SEO Generated!")
Â  Â  Â  Â  Â  Â  Â  Â  st.code(seo_result, language="markdown")
Â  Â  else:
Â  Â  Â  Â  st.info("Please generate a script in Tab 1 first.")
