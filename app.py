import warnings
warnings.filterwarnings("ignore")
import os
os.environ["GRPC_VERBOSITY"] = "ERROR"
os.environ["GLOG_minloglevel"] = "2"

import streamlit as st
import google.generativeai as genai
import edge_tts
import asyncio
import subprocess
import json
import time
import shutil
import whisper
import re
from pydub import AudioSegment
from PIL import Image
from google.api_core import exceptions

# ---------------------------------------------------------
# üíæ STATE MANAGEMENT
# ---------------------------------------------------------
if 'processed_video' not in st.session_state: st.session_state.processed_video = None
if 'generated_script' not in st.session_state: st.session_state.generated_script = ""
if 'api_key' not in st.session_state: st.session_state.api_key = ""

# ---------------------------------------------------------
# üé® UI SETUP
# ---------------------------------------------------------
st.set_page_config(page_title="Myanmar AI Studio Pro", page_icon="üá≤üá≤", layout="wide")
st.markdown("""
    <style>
    .stApp { background-color: #0e1117; color: #ffffff; }
    .stButton>button {
        background: linear-gradient(90deg, #00C9FF, #92FE9D); 
        color: black; border: none; height: 50px; font-weight: bold; width: 100%;
        border-radius: 10px; font-size: 16px;
    }
    .status-box { padding: 15px; border-radius: 10px; margin-bottom: 10px; border: 1px solid #444; background: #222; }
    textarea { font-size: 16px !important; }
    </style>
""", unsafe_allow_html=True)

# ---------------------------------------------------------
# üõ†Ô∏è HELPER FUNCTIONS & AUDIO ENGINE
# ---------------------------------------------------------
def check_requirements():
    if shutil.which("ffmpeg") is None:
        st.error("‚ùå FFmpeg is missing."); st.stop()

def get_duration(path):
    try:
        cmd = ['ffprobe', '-v', 'error', '-show_entries', 'format=duration', '-of', 'json', path]
        r = subprocess.run(cmd, capture_output=True, text=True)
        return float(json.loads(r.stdout)['format']['duration'])
    except: return 0

async def tts_save(text, voice, rate, pitch, output_file):
    # Edge-TTS requires specific format for rate/pitch (e.g., +10%, -5Hz)
    communicate = edge_tts.Communicate(text, voice, rate=rate, pitch=pitch)
    await communicate.save(output_file)

def generate_audio(text, voice_id, rate, pitch, filename):
    try:
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        loop.run_until_complete(tts_save(text, voice_id, rate, pitch, filename))
        loop.close()
        return True
    except: return False

# ---------------------------------------------------------
# üõ°Ô∏è SMART TRANSLATION & TEXT CLEANER
# ---------------------------------------------------------
def clean_and_normalize_burmese(text):
    # 1. Remove English characters to prevent TTS failure
    cleaned = re.sub(r'[A-Za-z]', '', text)
    
    # 2. Normalize common abbreviations (Smart Reader)
    # Note: These replacements should ideally happen BEFORE removing English, 
    # but since AI translation handles most, we use this as a cleanup backup for Burmese numbers/symbols.
    replacements = {
        "·ÅÄ": "·Äû·ÄØ·Ää", "·ÅÅ": "·Äê·ÄÖ·Ä∫", "·ÅÇ": "·Äî·Äæ·ÄÖ·Ä∫", "·ÅÉ": "·Äû·ÄØ·Ä∂·Ä∏", "·ÅÑ": "·Äú·Ä±·Ä∏", 
        "·ÅÖ": "·ÄÑ·Ä´·Ä∏", "·ÅÜ": "·ÄÅ·Äº·Ä±·Ä¨·ÄÄ·Ä∫", "·Åá": "·ÄÅ·ÄØ·Äî·ÄÖ·Ä∫", "·Åà": "·Äõ·Äæ·ÄÖ·Ä∫", "·Åâ": "·ÄÄ·Ä≠·ÄØ·Ä∏",
        "%": "·Äõ·Ä¨·ÄÅ·Ä≠·ÄØ·ÄÑ·Ä∫·Äî·Äæ·ÄØ·Äî·Ä∫·Ä∏", "/": "·Äô·Äª·Äâ·Ä∫·Ä∏·ÄÖ·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏", "+": "·Ä°·Äï·Ä±·Ä´·ÄÑ·Ä∫·Ä∏"
    }
    for k, v in replacements.items():
        cleaned = cleaned.replace(k, v)
        
    return cleaned.strip()

def translate_strict(model, text, style):
    prompt = f"""
    Act as a professional Burmese Dubbing Scriptwriter.
    Translate the following English text to spoken Burmese.
    
    Input: "{text}"
    
    CRITICAL INSTRUCTIONS:
    1. **Language**: OUTPUT BURMESE ONLY. No English words.
    2. **Numbers & Units**: Write them out phonetically.
       - "No.1" -> "·Äî·Ä∂·Äï·Ä´·Äê·Ä∫ ·Äê·ÄÖ·Ä∫"
       - "10,000" -> "·Äê·ÄÖ·Ä∫·Äû·Ä±·Ä¨·ÄÑ·Ä∫·Ä∏"
       - "50kg" -> "·ÄÑ·Ä´·Ä∏·ÄÜ·Äö·Ä∫ ·ÄÄ·ÄÆ·Äú·Ä≠·ÄØ·ÄÇ·Äõ·Äô·Ä∫"
       - "100mm" -> "·Äê·ÄÖ·Ä∫·Äõ·Ä¨ ·Äô·ÄÆ·Äú·ÄÆ·Äô·ÄÆ·Äê·Ä¨"
    3. **Tone/Style**: {style}.
       - If 'Movie Recap': Use dramatic, storytelling words.
       - If 'News': Use formal, objective words.
       - If 'Deep': Use serious, weighted words.
    4. **Output**: Just the Burmese text.
    """
    
    retries = 3
    for attempt in range(retries):
        try:
            response = model.generate_content(prompt)
            translated = response.text.strip()
            
            # Filter Check
            if re.search(r'[\u1000-\u109F]', translated): # Check for Burmese chars
                return clean_and_normalize_burmese(translated)
            continue
        except exceptions.ResourceExhausted:
            time.sleep(5)
            continue
        except:
            continue
    return ""

# ---------------------------------------------------------
# üé¨ VIDEO DUBBING WORKFLOW
# ---------------------------------------------------------
def process_video_workflow(video_path, voice_config, style, api_key, model_name, status, progress):
    check_requirements()
    
    # Unpack Voice Config (Advanced Tone Control)
    voice_id = voice_config["id"]
    pitch = voice_config["pitch"]
    rate = voice_config["rate"]
    
    # 1. Extract Audio
    status.info("üéß Extracting Audio...")
    subprocess.run(['ffmpeg', '-y', '-i', video_path, '-vn', '-acodec', 'pcm_s16le', '-ar', '16000', '-ac', '1', 'temp.wav'], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    
    # 2. Whisper
    status.info("üß† Listening (Whisper)...")
    whisper_model = whisper.load_model("base")
    result = whisper_model.transcribe("temp.wav")
    segments = result['segments']
    
    # 3. Translate & Dub
    status.info(f"üéôÔ∏è Dubbing {len(segments)} segments in '{style}' style...")
    genai.configure(api_key=api_key)
    try: model = genai.GenerativeModel(model_name)
    except: model = genai.GenerativeModel("gemini-1.5-flash")

    final_audio = AudioSegment.silent(duration=get_duration(video_path) * 1000)
    total = len(segments)
    
    for i, seg in enumerate(segments):
        start, end = seg['start'], seg['end']
        
        # Rate Limit
        time.sleep(2)
        
        # Translate
        burmese_text = translate_strict(model, seg['text'], style)
        
        if not burmese_text:
            progress.progress((i + 1) / total)
            continue
            
        # TTS Generation
        fname = f"seg_{i}.mp3"
        success = generate_audio(burmese_text, voice_id, rate, pitch, fname)
        
        if success and os.path.exists(fname):
            seg_audio = AudioSegment.from_file(fname)
            curr_dur = len(seg_audio) / 1000.0
            target_dur = end - start
            
            if curr_dur > 0 and target_dur > 0:
                # Time Stretch (Sync)
                speed = max(0.6, min(curr_dur / target_dur, 1.5))
                subprocess.run(['ffmpeg', '-y', '-i', fname, '-filter:a', f"atempo={speed}", f"s_{i}.mp3"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                
                if os.path.exists(f"s_{i}.mp3"):
                    final_seg = AudioSegment.from_file(f"s_{i}.mp3")
                    final_audio = final_audio.overlay(final_seg, position=start * 1000)
                    try: os.remove(f"s_{i}.mp3")
                    except: pass
            try: os.remove(fname)
            except: pass
            
        progress.progress((i + 1) / total)

    status.info("üîä Mixing Audio...")
    final_audio.export("final_track.mp3", format="mp3")
    
    status.info("üé¨ Finalizing...")
    output_filename = f"dubbed_{int(time.time())}.mp4"
    subprocess.run(['ffmpeg', '-y', '-i', video_path, '-i', "final_track.mp3", '-c:v', 'copy', '-c:a', 'aac', '-map', '0:v:0', '-map', '1:a:0', output_filename], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    
    return output_filename

# ---------------------------------------------------------
# üìù SCRIPT GENERATOR & TEXT-TO-SPEECH
# ---------------------------------------------------------
def generate_script(topic, script_type, user_prompt, api_key, model_name):
    genai.configure(api_key=api_key)
    try: model = genai.GenerativeModel(model_name)
    except: model = genai.GenerativeModel("gemini-1.5-flash")
    
    prompt = f"""
    Act as a professional Burmese Content Writer.
    Write a '{script_type}' script about: "{topic}".
    
    User Instructions: {user_prompt}
    
    RULES:
    1. Language: Burmese (Myanmar).
    2. Format: Clear, engaging, and ready to read.
    3. Length: Medium (approx 300 words).
    """
    
    response = model.generate_content(prompt)
    return response.text

def script_to_audio(script_text, voice_config):
    # Clean text first
    clean_text = clean_and_normalize_burmese(script_text)
    
    # Split into chunks (Edge-TTS has limits)
    chunks = [clean_text[i:i+500] for i in range(0, len(clean_text), 500)]
    combined = AudioSegment.empty()
    
    for idx, chunk in enumerate(chunks):
        fname = f"script_part_{idx}.mp3"
        generate_audio(chunk, voice_config["id"], voice_config["rate"], voice_config["pitch"], fname)
        if os.path.exists(fname):
            combined += AudioSegment.from_file(fname)
            os.remove(fname)
            
    output_path = f"script_audio_{int(time.time())}.mp3"
    combined.export(output_path, format="mp3")
    return output_path

# ---------------------------------------------------------
# üñ•Ô∏è MAIN UI
# ---------------------------------------------------------
with st.sidebar:
    st.title("üá≤üá≤ AI Studio Pro")
    api_key = st.text_input("API Key", type="password", value=st.session_state.api_key)
    if api_key: st.session_state.api_key = api_key
    
    st.divider()
    model_choice = st.selectbox("Model", ["gemini-1.5-flash", "gemini-2.0-flash-exp", "gemini-1.5-pro"])
    
    if st.button("üî¥ Reset App"):
        st.session_state.processed_video = None
        st.session_state.generated_script = ""
        st.rerun()

if not st.session_state.api_key:
    st.warning("Please enter API Key."); st.stop()

# TABS
t1, t2, t3, t4 = st.tabs(["üéôÔ∏è Video Dubbing", "üìù Script Writer", "üöÄ Viral Kit", "üñºÔ∏è Thumbnail"])

# --- TAB 1: DUBBING ---
with t1:
    st.subheader("üîä Video Dubbing")
    uploaded_file = st.file_uploader("üìÇ Upload Video", type=['mp4', 'mov'])
    
    if uploaded_file:
        with open("temp.mp4", "wb") as f: f.write(uploaded_file.getbuffer())

    # Voice & Tone Controls
    c1, c2, c3 = st.columns(3)
    with c1: narrator = st.selectbox("Narrator", ["Male (Thiha)", "Female (Nilar)"])
    with c2: 
        tone = st.selectbox("Voice Tone", [
            "Normal (Default)", 
            "Movie Recap (Dramatic)", 
            "News (Formal)", 
            "Deep (Serious)", 
            "Calming (Soft)",
            "Fast (Excited)"
        ])
    
    # Define Voice Configs based on Tone
    # Base Voices
    base_id = "my-MM-ThihaNeural" if "Male" in narrator else "my-MM-NilarNeural"
    
    # Tone Settings (Pitch/Rate adjustments)
    if tone == "Movie Recap (Dramatic)":
        v_conf = {"id": base_id, "rate": "+0%", "pitch": "-5Hz"}
        style_prompt = "Dramatic, Storytelling"
    elif tone == "News (Formal)":
        v_conf = {"id": base_id, "rate": "+5%", "pitch": "+0Hz"}
        style_prompt = "Formal, Professional"
    elif tone == "Deep (Serious)":
        v_conf = {"id": base_id, "rate": "-10%", "pitch": "-15Hz"}
        style_prompt = "Serious, Intense"
    elif tone == "Calming (Soft)":
        v_conf = {"id": base_id, "rate": "-5%", "pitch": "+5Hz"}
        style_prompt = "Soft, Gentle"
    elif tone == "Fast (Excited)":
        v_conf = {"id": base_id, "rate": "+20%", "pitch": "+10Hz"}
        style_prompt = "Excited, Energetic"
    else: # Normal
        v_conf = {"id": base_id, "rate": "+0%", "pitch": "+0Hz"}
        style_prompt = "Natural Conversation"

    if uploaded_file and st.button("üöÄ Start Dubbing"):
        st.session_state.processed_video = None # Clear old
        status = st.empty()
        prog = st.progress(0)
        try:
            out = process_video_workflow("temp.mp4", v_conf, style_prompt, st.session_state.api_key, model_choice, status, prog)
            st.session_state.processed_video = out
            st.success("Dubbing Complete!")
            st.rerun()
        except Exception as e:
            st.error(f"Error: {e}")
            
    if st.session_state.processed_video:
        st.video(st.session_state.processed_video)
        with open(st.session_state.processed_video, "rb") as f:
            st.download_button("Download Video", f, "dubbed.mp4")

# --- TAB 2: SCRIPT WRITER ---
with t2:
    st.subheader("üìù Smart Script Generator")
    
    sc1, sc2 = st.columns(2)
    with sc1:
        script_type = st.selectbox("Script Type", ["Story", "Movie Script", "Video Voiceover Script", "Documentary"])
    with sc2:
        script_tone = st.selectbox("Script Tone", ["Emotional", "Educational", "Funny", "Serious"])
        
    topic = st.text_input("Topic (e.g., History of Bagan)")
    user_prompt = st.text_area("Specific Instructions (Optional)", placeholder="E.g., Start with a question, focus on the architecture...")
    
    if st.button("‚úçÔ∏è Generate Script"):
        if not topic: st.warning("Please enter a topic.")
        else:
            with st.spinner("Writing Script..."):
                script = generate_script(topic, script_type, user_prompt, st.session_state.api_key, model_choice)
                st.session_state.generated_script = script
                st.rerun()
                
    if st.session_state.generated_script:
        st.markdown("### Generated Script")
        st.text_area("Edit Script if needed:", value=st.session_state.generated_script, height=300, key="script_editor")
        
        st.markdown("### üîä Convert Script to Audio")
        # Audio Settings for Script
        ac1, ac2 = st.columns(2)
        with ac1: s_voice = st.selectbox("Audio Voice", ["Male (Thiha)", "Female (Nilar)"], key="s_voice")
        with ac2: s_tone = st.selectbox("Audio Tone", ["Normal", "Deep", "News"], key="s_tone")
        
        # Map Script Tone
        s_base = "my-MM-ThihaNeural" if "Male" in s_voice else "my-MM-NilarNeural"
        if s_tone == "Deep": s_conf = {"id": s_base, "rate": "-10%", "pitch": "-10Hz"}
        elif s_tone == "News": s_conf = {"id": s_base, "rate": "+5%", "pitch": "+0Hz"}
        else: s_conf = {"id": s_base, "rate": "+0%", "pitch": "+0Hz"}
        
        if st.button("üó£Ô∏è Read Script"):
            with st.spinner("Generating Audio..."):
                final_text = st.session_state.script_editor # Use edited text
                audio_path = script_to_audio(final_text, s_conf)
                st.audio(audio_path)
                with open(audio_path, "rb") as f:
                    st.download_button("Download Audio", f, "script_audio.mp3")

# --- TAB 3 & 4 (Viral & Thumbnail - Placeholder for specific code above) ---
with t3:
    st.info("Viral Kit features are integrated in main logic.")
with t4:
    st.info("Thumbnail features are integrated in main logic.")
